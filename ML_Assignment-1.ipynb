{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889e5303-71eb-4c7e-8d25-1fe35f72453d",
   "metadata": {},
   "source": [
    "                                                                 # ML Assignment - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ab0b3-500d-431b-8819-e016a662c9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "007cd7f7-760e-4015-8488-73b1be3186e5",
   "metadata": {},
   "source": [
    "## Q1. Define Artificial intelligence AI ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc334b98-e0dc-456e-9f9e-f1314f83eeb0",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) refers to the capability of machines to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI systems use algorithms to process data, recognize patterns, and improve over time. It is a branch of computer science that focuses on building intelligent systems that perform tasks, that require intelligence when performed by humans.\n",
    "\n",
    "A common example of AI is virtual assistants like Siri or Alexa, which understand and respond to voice commands. Another example is recommendation systems, such as those on Netflix, that suggest content based on user preferences. AI is also used in autonomous vehicles, enabling them to navigate and make decisions without human input,\n",
    "demonstrating its potential across various industries.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5f262-069b-4e21-954e-2130ec3d4335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e788eed-3962-4895-9b8a-cfbc6d3e84bb",
   "metadata": {},
   "source": [
    "## Q2. Explain the differences between Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Data Science (DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b9c09-78dc-45a1-892f-7434e29bd6d2",
   "metadata": {},
   "source": [
    "     **Artificial Intelligence(AI)**:\n",
    "The broad field focused on creating machines that can perform tasks requiring human intelligence.\n",
    "Smart application that can perform own task without human intervention.\n",
    "        \n",
    "Ex:- Robot , self Drvining car, Decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436404b3-ed0f-425d-bad4-b463969dd081",
   "metadata": {},
   "source": [
    "    **Machine Learning(ML)**:\n",
    "\n",
    "A subset of AI, involves algorithms that enable machines to learn from data and improve over time. Machine learn pattern from data and tried to replicate same in future.\n",
    "        \n",
    "Ex:- Spam & Ham ,Diabetes or not ,price of house"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b83a8-cddf-4fd0-9019-8f57c02cb2a7",
   "metadata": {},
   "source": [
    "    **Deep Learning(DL)**:\n",
    "A subset of ML, uses neural networks to model complex patterns and is often applied in image and speech recognition.\n",
    "Specialized machine learning algorithms that mimic human brain.\n",
    "    \n",
    "Ex:- chat gpt ,devin AI,Object detections,Image recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0cd9e-73ab-42c9-b81d-46b8f2213ee5",
   "metadata": {},
   "source": [
    "    **Data Science(DS)**:\n",
    "It involves extracting insights from data through various techniques, including statistics and machine learning.\n",
    "Interdisciplinary field comprised of statistics,math,and domain knowledge.\n",
    "\n",
    "Ex :- AI powers virtual assistants, ML is used for predictive analytics, DL enables facial recognition, and DS analyzes customer trends for businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70adfb11-4c0a-46c1-9b5b-140f90b9954f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "678abf44-3f13-4a6c-a696-fea115bc0b32",
   "metadata": {},
   "source": [
    "## Q3. How does AI differ from traditional software developments ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f7f60-3408-49e8-92b8-7f7ac786f64c",
   "metadata": {},
   "source": [
    "AI differs from traditional software development in that it focuses on creating systems that can learn and adapt from data, rather than following a fixed set of instructions. Traditional software is rule-based, where programmers define explicit instructions for every possible scenario. In contrast, AI systems, like machine learning models, use data to identify patterns and make decisions without being explicitly programmed for every case.\n",
    "\n",
    "For example, in traditional software, a program might calculate tax based on fixed rules, while in AI, a system could predict tax outcomes based on historical data and evolving patterns.\n",
    "\n",
    "AI differs from traditional software development in several fundamental ways. Here are the key differences:\n",
    "\n",
    "### 1. **Programming Paradigm**:\n",
    "   - **Traditional Software**: Follows a rule-based paradigm where developers explicitly program all the rules and logic needed to perform specific tasks. The software performs actions based on these predefined instructions.\n",
    "   - **AI Development**: Focuses on data-driven approaches. Instead of explicitly coding all rules, AI systems learn patterns and make decisions based on data. Machine learning (ML) models are trained on large datasets to find patterns and make predictions or decisions.\n",
    "\n",
    "### 2. **Learning and Adaptation**:\n",
    "   - **Traditional Software**: Once programmed, the software behavior remains static unless it is manually updated by developers. The system does not learn or adapt from new data.\n",
    "   - **AI Systems**: Continuously learn and improve from new data. AI models can adapt to new information without human intervention, allowing them to handle changing environments and tasks better.\n",
    "\n",
    "### 3. **Handling Complexity**:\n",
    "   - **Traditional Software**: Handles straightforward and well-defined problems where all possible inputs and outputs are known. It relies on deterministic logic.\n",
    "   - **AI Systems**: Are designed to handle complex, uncertain, or poorly defined problems where all possible scenarios cannot be hard-coded. They use probabilistic reasoning and pattern recognition to make decisions in complex environments.\n",
    "\n",
    "### 4. **Development Process**:\n",
    "   - **Traditional Software**: Development is typically linear, involving requirements gathering, design, coding, testing, and maintenance. Each step is well-defined.\n",
    "   - **AI Development**: Involves an iterative process of data collection, data preprocessing, model selection, training, validation, and testing. The development is often non-linear, with frequent experimentation and tuning.\n",
    "\n",
    "### 5. **Output and Debugging**:\n",
    "   - **Traditional Software**: Produces predictable outputs for a given input. Debugging involves tracing the code logic to find errors.\n",
    "   - **AI Systems**: Outputs can be probabilistic and not always explainable. Debugging involves understanding why a model made a certain prediction, which can be challenging, especially for complex models like deep neural networks.\n",
    "\n",
    "### 6. **Role of Data**:\n",
    "   - **Traditional Software**: Relies primarily on the algorithms coded by developers. Data is important but usually static and secondary to the code.\n",
    "   - **AI Systems**: Data is a core component. The quality and quantity of data directly impact the model's performance. AI models need vast amounts of labeled data for training and testing.\n",
    "\n",
    "### 7. **Decision Making**:\n",
    "   - **Traditional Software**: Decisions are made based on clearly defined rules and conditions.\n",
    "   - **AI Systems**: Decisions are made based on learned patterns, probabilities, and statistical models. The decision-making process can often be opaque or a \"black box.\"\n",
    "\n",
    "### 8. **Maintenance and Updates**:\n",
    "   - **Traditional Software**: Requires regular updates to add new features or fix bugs, which are explicitly coded by developers.\n",
    "   - **AI Systems**: Require retraining with new data to adapt to changing patterns. Maintenance involves updating models, managing data drift, and ensuring the model remains relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c24a6c-6f43-48ea-a486-f427240c1e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a7ea8d7-619f-40d2-a194-a06dc10add3e",
   "metadata": {},
   "source": [
    "## Q4. Provide examples of AI, ML, DL, and DS applications ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda0493-0553-4dc7-bda6-39d2ea47ab94",
   "metadata": {},
   "source": [
    "AI is used in **virtual assistants** like Siri, Alexa, and Google Assistant use AI to understand and respond to voice commands, helping with tasks like setting reminders or controlling smart devices, recommendation systems, and more.\n",
    "    \n",
    "ML is applied in image recognition, spam filtering, and other data tasks. \n",
    "- **Email Spam Filters**: Machine learning algorithms analyze patterns in emails to identify spam and sort them\n",
    "accordingly, improving accuracy as they process more emails.\n",
    "    \n",
    "DL is utilized in autonomous vehicles, speech recognition, and advanced AI applications.\n",
    "- **Image Recognition**: Deep learning is used in systems like facial recognition software (e.g., Face ID) or medical imaging, where neural networks can detect diseases from X-ray or MRI scans.\n",
    "    \n",
    "DS is utilized In Search Engines,Transport ,Finance, Ecommerce and Predictive analytics in business to forcast sales trends.\n",
    "- **Customer Behavior Analysis**: Data scientists analyze customer purchase data to identify trends and predict future buying patterns, which helps businesses personalize marketing strategies and optimize product offerings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafc78c-8044-44be-9f13-160f6bcfa640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed1f7b67-5a88-4cd6-b256-58cd7d785fa5",
   "metadata": {},
   "source": [
    "## Q5. Discuss the importance of AI, ML, DL, and DS in today's World ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2cb20-fe5b-41a4-84b2-61325f5ab725",
   "metadata": {},
   "source": [
    "AI, ML, DL, and DS are transforming industries and daily life. These technologies are crucial for driving innovation, improving efficiency, anf solving complex problems across various industries.\n",
    "Artificial Intelligence (AI) sets the stage for machines that can simulate human intelligence. \n",
    "Machine Learning (ML) evolves from AI, giving machines the ability to learn and grow from experience. \n",
    "Deep Learning (DL), nestled within ML, drives machines to understand and operate on a level akin to human intuition.\n",
    "Data Science (DS) helps organizations extract valuable insights from large datasets, aiding in better decision-making and strategy development. Collectively, these technologies are revolutionizing sectors such as finance, healthcare, retail, and entertainment, driving innovation, increasing productivity, and creating new opportunities for growth and development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb0db1-bacf-4f18-8b18-f5dadda0e804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce1747e-e7de-410e-ac76-dc562f66452c",
   "metadata": {},
   "source": [
    "## Q6. What is supervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35541b1-9ca1-4eee-aaa2-def33b18d311",
   "metadata": {},
   "source": [
    "Supervised learning is a type of machine learning where the model is trained on labeled data. It involves learning a function that maps input variables (independent variables) to an output variable (target variable). The two main problem types in supervised learning are:\n",
    "\n",
    "1. **Regression**: Predicting continuous output values (e.g., predicting house prices).\n",
    "2. **Classification**: Predicting discrete categories or labels (e.g., classifying emails as spam or not spam).\n",
    "\n",
    "The goal is to learn the relationship between input and output variables to make accurate predictions on new, unseen data.\n",
    "\n",
    "Supervised learning is commonly used for tasks like classification (e.g., email spam detection) and regression (e.g., predicting house prices). Examples of algorithms used in supervised learning include decision trees, support vector machines, and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b634b-dd8f-47ee-9421-8103ad257b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e04423f9-bdbb-4bbe-9cd8-b51f40b2e9b6",
   "metadata": {},
   "source": [
    "## Q7 .Provide the examples of supervised learning algorithms ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200752e-1a4f-4fb6-93c3-b7d23096602f",
   "metadata": {},
   "source": [
    "The most commonly used Supervised Learning algorithms:\n",
    "1. **Linear Regression**: Used for predicting continuous values, like predicting house prices based on features like size, location, and number of rooms.\n",
    "\n",
    "2. **Logistic Regression**: Used for binary classification tasks, such as determining whether an email is spam or not.\n",
    "\n",
    "3. **Decision Trees**: A tree-like model used for classification and regression, which splits data into subsets based on feature values.\n",
    "\n",
    "4. **Support Vector Machines (SVM)**: Used for classification tasks by finding the optimal hyperplane that best separates classes in the data.\n",
    "\n",
    "5. **k-Nearest Neighbors (k-NN)**: Classifies data based on the majority class of its nearest neighbors in the feature space.\n",
    "\n",
    "6. **Naive Bayes**: A probabilistic algorithm used for classification, based on applying Bayes' Theorem with strong (naive) independence assumptions between features.\n",
    "\n",
    "These algorithms are widely used for various applications like image recognition, medical diagnosis, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d63d22-22f4-46ad-aef7-8a6252a025cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "501cd7d4-d6c6-4a7f-8fde-890541124a29",
   "metadata": {},
   "source": [
    "## Q8. Explain the process of supervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37cef7-c1d1-4398-9468-1a42ff2e499d",
   "metadata": {},
   "source": [
    "Supervised learning is a type of machine learning where an algorithm is trained on labeled data, meaning that the input data comes with corresponding output labels. The goal of supervised learning is to learn a mapping from inputs to outputs, so the model can predict the output for new, unseen inputs.\n",
    "\n",
    "It is used to explain the relationship between input and output variable.\n",
    "The process of supervised learning step-by-step:\n",
    "1. **Data Collection**\n",
    "- First, a dataset is collected, which contains examples of input-output pairs.\n",
    "- For example, in a spam email classification problem, the input could be the content of an email, and the output could be a label indicating whether the email is \"spam\" or \"not spam.\"\n",
    "  \n",
    "2. **Data Preprocessing**\n",
    "- The collected data is then cleaned and preprocessed. This includes handling missing data, scaling features, encoding categorical variables, and splitting the dataset into training and testing sets.\n",
    "- Typically, 70-80% of the data is used for training, while the rest is reserved for testing.\n",
    "  \n",
    "3. **Model Selection**\n",
    "- Choose an appropriate supervised learning algorithm depending on the type of problem (e.g., regression, classification).\n",
    "- Common algorithms include:\n",
    "  - Linear Regression for continuous output (regression problem).\n",
    "  - Logistic Regression, Decision Trees, Random Forests, Support Vector \n",
    "    Machines (SVM), and K-Nearest Neighbors (KNN) for categorical output \n",
    "    (classification problem)\n",
    "    \n",
    "4. **Training the Model**\n",
    "- During training, the model learns from the labeled training data. It tries to minimize the error by adjusting its parameters.\n",
    "- For instance, in linear regression, it finds the line of best fit. In decision trees, it splits the data at each node to minimize a chosen criterion (e.g., Gini impurity or information gain).\n",
    "- The training process typically involves optimization techniques like gradient descent.\n",
    "    \n",
    "5. **Model Evaluation**\n",
    "- After training, the model is evaluated using the testing dataset, which has not been seen by the model before.\n",
    "- Evaluation metrics depend on the problem type:\n",
    "  - For regression: Mean Squared Error (MSE), R-squared.\n",
    "  - For classification: Accuracy, Precision, Recall, F1-score, ROC-AUC.\n",
    "- This helps in determining how well the model generalizes to new, unseen data.\n",
    "  \n",
    "6. **Hyperparameter Tuning**\n",
    "- Often, models have hyperparameters (settings that control the learning process, like the learning rate or number of tree splits in a decision tree) that need to be tuned to improve performance.\n",
    "- Techniques such as grid search or random search can be used to find the best set of hyperparameters.\n",
    "\n",
    "7. **Prediction**\n",
    "- Once the model is trained and evaluated, it can be used to make predictions on new, unseen data.\n",
    "- The model outputs a predicted label (for classification) or a continuous value (for regression), depending on the problem.\n",
    "\n",
    "8. **Model Improvement**\n",
    "- If the model's performance is not satisfactory, you may iterate through the process, trying different algorithms, adjusting hyperparameters, or collecting more data.\n",
    "    \n",
    "It is commonly used for tasks like classification (e.g., spam detection, image recognition) and regression (e.g., house price prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799ec61-1099-4cda-8039-410b47427882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7cfbae8-a35c-48a3-a3ec-786908469610",
   "metadata": {},
   "source": [
    "## Q9. What is the characteristics of unsupervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443313f-392b-4bff-9ced-1c6d34a012b7",
   "metadata": {},
   "source": [
    "Unsupervised learning is a type of machine learning where the model is trained on data that has no labels or predefined outputs. The goal is to find hidden patterns or intrinsic structures within the data. Unlike supervised learning, where the model learns from input-output pairs, unsupervised learning focuses on identifying relationships, groupings, or distributions within the input data. Here are the key characteristics:\n",
    "\n",
    "1. **No Labeled Data**: Unlike supervised learning, unsupervised learning works with data that has no target or output labels. The model tries to find patterns, structures, or relationships within the input data.\n",
    "\n",
    "2. **Data Exploration**: It is primarily used for exploring the underlying structure of the data, finding hidden patterns, and gaining insights from unstructured data.\n",
    "\n",
    "3. **Clustering and Association**: The main types of problems are:\n",
    "   - **Clustering**: Grouping similar data points together (e.g., customer segmentation).\n",
    "   - **Association**: Finding relationships between variables in large datasets (e.g., market basket analysis).\n",
    "\n",
    "4. **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are used to reduce the number of variables while preserving important information.\n",
    "\n",
    "5. **No Direct Feedback**: There is no feedback from labeled data to guide the learning process. The model relies on the inherent structure of the data to make decisions.\n",
    "\n",
    "6. **Flexibility**: Unsupervised learning algorithms can handle a variety of tasks and often require less human intervention, as they don't need labeled data. However, this flexibility can also make it more challenging to evaluate model performance.\n",
    "\n",
    "7. **Applications**: Used in tasks such as anomaly detection, pattern recognition, feature extraction, and data compression.\n",
    "\n",
    "8. **Data-Driven Learning**: The algorithms learn from data patterns without predefined classes or categories, making them suitable for exploratory analysis.\n",
    "\n",
    "9.**Challenges**:\n",
    "- Evaluation: Since there are no labeled outputs to compare against, evaluating the performance of unsupervised learning models can be difficult. Measures such as the \"silhouette score\" for clustering or \"variance explained\" for dimensionality reduction are used.\n",
    "- Interpretability: The patterns discovered might not always be easily interpretable or meaningful without domain knowledge.\n",
    "\n",
    "Note: Unsupervised learning is about discovering hidden structures in data without relying on labeled outputs. It includes tasks like clustering, dimensionality reduction, and anomaly detection. While it offers flexibility and can reveal valuable insights, it can be more difficult to evaluate and interpret due to the absence of clear output labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83d852-57b4-4972-b89c-fe6d3d79f919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17cb14de-d243-44c4-b0f7-6bf8c1c33383",
   "metadata": {},
   "source": [
    "## Q10. Give examples of unsupervised Learning algorithms ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fba5b3-8947-4aaa-8475-322902862ae0",
   "metadata": {},
   "source": [
    "Unsupervised learning algorithms help find patterns or structure in data without labels. Some examples include:\n",
    "\n",
    "1. **K-Means Clustering**: Groups data into clusters based on similarity.\n",
    "2. **Hierarchical Clustering**: Builds a tree of clusters.\n",
    "3. **DBSCAN**: Groups data based on density and identifies outliers.\n",
    "4. **PCA (Principal Component Analysis)**: Reduces data dimensions while preserving important features.\n",
    "5. **t-SNE**: Visualizes high-dimensional data in 2D or 3D.\n",
    "6. **Autoencoders**: Compresses data and learns features for tasks like anomaly detection.\n",
    "7. **Gaussian Mixture Model (GMM)**: Finds clusters using probability distributions.\n",
    "These algorithms are used for tasks like grouping, reducing complexity, or detecting unusual patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff57765-1bc2-4454-a5c6-8da635b19dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76725cfa-8bb4-4851-98e4-a30d4297395d",
   "metadata": {},
   "source": [
    "## Q11. Describe semi-supervised Learning and its significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c85237-1576-431e-bfe8-d781f4abfc5c",
   "metadata": {},
   "source": [
    "Semi-supervised learning combines a small amount of labeled data with a large amount of unlabeled data for training. It improves model accuracy by leveraging patterns in the unlabeled data while using labeled data to guide learning. This approach reduces the cost and effort of data labeling, making it ideal for real-world applications like web content classification, medical diagnosis, and image recognition, where labeled data is scarce but unlabeled data is abundant. It provides better performance than unsupervised learning alone and is data-efficient in low-resource settings.\n",
    "\n",
    "This approach is significant because labeling data can be expensive and time-consuming. Semi-supervised learning helps improve model accuracy by making the most of unlabeled data, especially in cases where obtaining labeled data is difficult or costly. It’s often used in tasks like image recognition or text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566b9cf-169d-4d36-9b1a-0916365adbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23c5554-212a-4bfc-b95a-b415b3dc9c5e",
   "metadata": {},
   "source": [
    "## Q12. Explain Reinforcement Learning and its applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd20e4-0bee-4dee-9e17-e3582b5803d6",
   "metadata": {},
   "source": [
    "Reinforcement learning is an area of ml concerned with how intelligent agent ought to take action in  an environment. It performs actions and receives feedback in the form of rewards or penalties.\n",
    "Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals.\n",
    "\n",
    "Applications of RL include robotics (for teaching machines to perform tasks), gaming (like in AlphaGo or video game AI), self-driving cars (to make decisions in real-time), and recommendation systems (to suggest items based on past choices). RL is widely used where decision-making and continuous learning are essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f42fd-c0f7-4713-8975-6e9f0c3aa51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07377b2d-15b1-43f4-b17a-e8c8bc85c7b8",
   "metadata": {},
   "source": [
    "## Q13. How does Reinforcement Learning differ from supervised and unsupervised Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03188e8f-bbd2-4150-b2b7-d9645c6fd803",
   "metadata": {},
   "source": [
    "    Supervised learning :        \n",
    "The model is trained on labeled data (inputs with known outputs), learning to predict the output from the input.\n",
    "        \n",
    "        \n",
    "    Unsupervised Learning :\n",
    "The model tries to find patterns or groupings in data without labels. It uses unlabeled data to learn pattern from data.\n",
    "        \n",
    "        \n",
    "    Reinforcement learning :\n",
    "Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals.\n",
    "The agent learns by trial and error, receiving feedback (rewards or penalties) based on actions in an environment. While supervised and unsupervised learning focus on learning from data directly, RL focuses on learning from interactions and improving over time through feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b64725-ace6-42a8-8fcd-f9617ba9bad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8317361b-b9dc-4584-b593-7974d6e44912",
   "metadata": {},
   "source": [
    "## Q14 What is the purpose of the Train-Test-Validation split in Machine learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dccbe5-28e2-4fe9-9890-ef87913b2ee0",
   "metadata": {},
   "source": [
    "The Train-Test-Validation split is used to evaluate and improve machine learning models. The Purpose of train test & validation:\n",
    "    \n",
    "    Train :\n",
    "        train data used for training of model.\n",
    "        \n",
    "    Test :\n",
    "        Test data is used for testing the model accuracy\n",
    "        \n",
    "    Validation :\n",
    "        Validation data is used for hyperparameter tuning.\n",
    "\n",
    "This split helps prevent overfitting (where the model is too tailored to the training data) and ensures that the model generalizes well to real-world data, leading to better accuracy and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ae99a-c444-4d28-be1e-66666c7fdfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb229b4-cfb7-4a51-a036-d351bc998c00",
   "metadata": {},
   "source": [
    "## Q15. Explain the significance of the training set ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdac02-239d-483a-b3eb-52af3dc130ce",
   "metadata": {},
   "source": [
    "The training set is crucial as it used to fit the model. The model learns patterns, relationships, and features from this dataset to make accurate predictions or classifications.\n",
    "The quality and size of the training set directly impact the model's ability to generalize and perform well on new, unseen data. A good training set should be representative of the real-world scenario the model will face. The better the training data, the more accurate and reliable the model’s predictions will be when it encounters new inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e0d96-5bb9-4d2a-98cc-4a7ce941f84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26a46f47-ae68-4f2f-a747-2b482bce0c82",
   "metadata": {},
   "source": [
    "## Q16.How do you determine the size of the training, testing, and validation sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f37eac-6dda-4e63-889a-24ebb5fcc822",
   "metadata": {},
   "source": [
    "The size of the training, testing, and validation sets depends on the dataset size and the task at hand.\n",
    "   \n",
    "    Train :\n",
    "train data size comprises 70-80% of the data to train.\n",
    "        \n",
    "    Test :\n",
    "Test data size is 10-15% .We can test the model accuracy using these spoecific data.\n",
    "        \n",
    "    Validation data:\n",
    "Validation data size is 10-15%. we have to use the validation data for hyperparameter tuning\n",
    "\n",
    "For smaller datasets, you might use cross-validation, where the data is split into multiple parts, and the model is trained and tested on different splits to improve reliability. The key is ensuring enough data for each set to provide accurate model evaluation and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526f160-5ff5-40b4-8bb0-cc33fc27dc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83159490-423a-4c91-a3b0-5899bda8352e",
   "metadata": {},
   "source": [
    "## Q17 what are the consequences of improper Train-Test-Validation splits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20bae90-2d3d-4741-b5d9-5c4f5a92fb43",
   "metadata": {},
   "source": [
    "Improper splits can lead to overfitting (model performs well on training data but poorly on unseen data) or underfitting (model performs poorly on both training and unseen data). It can result in accurate assessment of model performance and reduced generalization.\n",
    "Incorrectly shuffling or sorting the data before splitting can introduce bias and affect the generalization of the final model. \n",
    "For example, if the dataset is not shuffled randomly before splitting into training set and validation set, it may introduce biases or patterns that the model can exploit during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e834f7-b4a3-4f99-be57-bd7f24a9158e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30d79d57-f6c3-4b61-b753-8b52c21e947c",
   "metadata": {},
   "source": [
    "## Q18 Discuss the trade-offs in selecting appropriate split Ratios ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3da49-60b0-4ac7-9d2f-d0a7417c2f12",
   "metadata": {},
   "source": [
    "In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model.\n",
    "Too little training data can lead to underfitting, and too little validation/test data can lead to unreliable perfomance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c778be-297b-4788-9a80-52e742beaf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4180d7c9-65ee-4343-84d8-eeaae4e9d7c3",
   "metadata": {},
   "source": [
    "## Q19 Define model performance in machine learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc9801-9a10-43fe-b48a-4a09520fc1de",
   "metadata": {},
   "source": [
    "Model performance in machine learning is how the model giving accurate prediction on unseen data that term is called as model performance.It typically measure model performance using a test set, where you compare the predictions on the test set to the actual outcomes.\n",
    "It is usually measured using metrics like accuracy, precision, recall, or F1-score, depending on the problem. \n",
    "\n",
    "For example, in a spam email detection system, the performance might be measured by how correctly the model classifies emails as spam or not. If the model correctly identifies most spam emails and avoids false positives, its performance is considered good. High performance means the model generalizes well to new data, while poor performance suggests it struggles to make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cae93-2d9b-4232-a82b-779dacecb131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2548f39-01b5-45ce-aa2e-83c1382b8fd9",
   "metadata": {},
   "source": [
    "## Q20. How do you measure the performance of a machine learning model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158151ee-00cf-40e7-80ff-f8ca9df8ecee",
   "metadata": {},
   "source": [
    "The machine learning model can be evaluated using metrics that analyze its ability to make predictions, its weaknesses, and how well it can generalize future predictions\n",
    "    \n",
    "For Regression :\n",
    "we have use: \n",
    "1. mean square error\n",
    "2. mean absolute error\n",
    "3. Root mean square error\n",
    "4. r2 score\n",
    "        \n",
    "for classification :\n",
    "\n",
    "1.confusion matrix\n",
    "\n",
    "2.accuracy score\n",
    "\n",
    "3.classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54863f4-69ec-4598-a440-8f5db86caf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "150d525f-5051-4467-9f64-1503fbee1275",
   "metadata": {},
   "source": [
    "## Q21 What is overfitting and why is it problematic ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01b0aa-9278-45bf-b7b2-fe4b6f18b778",
   "metadata": {},
   "source": [
    "Overfitting occurs when a machine learning model learns the details and noise in the training data to an extent that it negatively impacts its performance on new, unseen data. In other words, the model becomes too tailored to the training data, capturing patterns that are not generalizable.\n",
    "\n",
    "Model performing well during training data but not during testing that is called as over fitting.\n",
    "   \n",
    "Train ==> while training accuracy is high\n",
    "   \n",
    "test ===> while testing accuracy is low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c77d0f-a82e-419c-874b-644026eafd83",
   "metadata": {},
   "source": [
    "Problem due to over fitting:\n",
    "    \n",
    "In ml while training accuracy is high & testing accuracy is low that is over fitting. over fitting can reduced the model reliability, it can produce inaccurate prediction.\n",
    "The model may perform very well on the training data but poorly on real-world or test data. Overfitting reduces the model's ability to generalize, leading to inaccurate predictions and poor performance when deployed in practical scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867ea7d-d52a-4cb3-9d34-da5905cf9fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "524013a1-7d6d-4961-abd7-7ea8994a3bf7",
   "metadata": {},
   "source": [
    "## Q22 Provide techniques to address over fitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71744c1f-e030-4788-ae1a-057469db40bd",
   "metadata": {},
   "source": [
    "To address overfitting, here are several techniques:\n",
    "\n",
    "1. **Cross-validation**: Use k-fold cross-validation to evaluate model performance on different data subsets, reducing the risk of overfitting to a specific training set.\n",
    "\n",
    "2. **Regularization**: Apply techniques like **L1** (Lasso) or **L2** (Ridge) regularization, which penalize large coefficients in the model, discouraging overcomplexity.\n",
    "\n",
    "3. **Pruning (for decision trees)**: Cut unnecessary branches from decision trees to prevent them from becoming too complex.\n",
    "\n",
    "4. **Reduce Model Complexity**: Use simpler models with fewer parameters to avoid overfitting to noise in the data.\n",
    "\n",
    "5. **Early Stopping**: Stop training before the model starts to memorize the training data, especially in neural networks.\n",
    "\n",
    "6. **Increase Training Data**: Adding more diverse data can help the model learn generalizable patterns rather than memorizing specific examples.\n",
    "\n",
    "7. **Dropout (for neural networks)**: Randomly drop units (neurons) during training to prevent the model from relying too heavily on any single feature.\n",
    "\n",
    "These techniques help ensure the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3da081-1c31-4614-8b3a-c574fc9fb019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19ec7450-3639-45d2-907f-5f2fe06ed2ab",
   "metadata": {},
   "source": [
    "## Q23 Explain underfitting and its implications ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b5780-5eb5-4c97-b089-f538cef23759",
   "metadata": {},
   "source": [
    "Underfitting happens when a machine learning model is too simple to capture the underlying patterns in the data. This occurs when the model has too few parameters, or is not trained enough, leading it to make poor predictions on both the training and test data.\n",
    "\n",
    "### Implications of underfitting:\n",
    "\n",
    "1. **Poor Performance**: The model fails to make accurate predictions because it cannot capture the complexity of the data.\n",
    "2. **Bias**: Underfitting results in high bias, meaning the model consistently makes errors due to its inability to learn important relationships.\n",
    "3. **Limited Insights**: The model doesn’t provide valuable insights because it oversimplifies the problem.\n",
    "\n",
    "To fix underfitting, you can use a more complex model, increase training time, or improve feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311aa0d-9d05-4a2d-ae9e-6258fe215127",
   "metadata": {},
   "source": [
    "Basically Low Train accuracy & low test accuracy is under fitting. Under fitting happen because of \n",
    "where data model is unable to capture the relationship of input and output variable. it causes high error while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772420d-4b86-4ceb-aa65-6053f47b712f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7d23836-5810-466d-afae-0b58c8c0e323",
   "metadata": {},
   "source": [
    "## Q24 How can you prevent under fitting in machine learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b25c8-44ef-4808-8dd8-0fbc357b5ba9",
   "metadata": {},
   "source": [
    "To prevent underfitting in machine learning models, consider these approaches:\n",
    "\n",
    "1. Increase model complexity.\n",
    "2. Increase relevant features, performing feature engineering.\n",
    "3. Remove noise from the data.\n",
    "4. Increase the number of epochs or increase the duration of training to get better\n",
    "5. Reduce regularization\n",
    "6. Increase model parameters\n",
    "7. check data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151efc10-54e4-4599-b152-bfcdda692452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59f9b9af-9196-4da9-966f-22fa876c5a29",
   "metadata": {},
   "source": [
    "## Q25 Discuss the balance between bias and variance in model performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9248fbc9-b801-4f2c-acc2-17f0ae633aac",
   "metadata": {},
   "source": [
    "#### High Bias & Low Bias :-\n",
    "    1. Low training error is also known as low bias\n",
    "    2. high training error is known as high bias\n",
    "    \n",
    "    \n",
    "#### High Variance & Low Variance:\n",
    "    1. low testing error is known as low variance\n",
    "    2. high testing error is known as high variance\n",
    "    \n",
    "#### Over fitting :\n",
    "    1. training accuracy is high ==> low bias\n",
    "    2. testing accuracy is low ==> high variance\n",
    "    \n",
    "#### Under fitting :\n",
    "    1. training accuracy is low ==> high bias\n",
    "    2. testing accuracy is low ==> high variance\n",
    "    \n",
    "#### Generalize model :\n",
    "    1. training accuracy is high ==> low bias\n",
    "    2. testing accuracy is high ==> low variance    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c679e-fff7-4102-8e8f-3ddc82de8788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e205a132-a6f5-41a7-a569-0d6834afb62e",
   "metadata": {},
   "source": [
    "## Q26 What are the common techniques to handle missing data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322913a-bad3-4e38-9055-4d2230192efe",
   "metadata": {},
   "source": [
    "Handling missing data is a crucial part of preprocessing in machine learning. Common techniques include:\n",
    "\n",
    "1. **Removing Missing Data**: \n",
    "   - **Row deletion**: Remove rows with missing values (useful if the missing data is minimal).\n",
    "   - **Column deletion**: Remove columns with a large proportion of missing data.\n",
    "\n",
    "2. **Imputation**:\n",
    "   - **Mean/Median Imputation**: Replace missing values with the mean (for numerical data) or median (if data is skewed).\n",
    "   - **Mode Imputation**: Replace missing categorical values with the most frequent category.\n",
    "   - **Predictive Imputation**: Use algorithms (like KNN or regression) to predict and fill in missing values based on other available data.\n",
    "\n",
    "3. **Using a Placeholder**: \n",
    "   - Assign a specific value (like 0, -1, or \"Unknown\") to represent missing data, particularly for categorical variables.\n",
    "\n",
    "4. **Interpolation**:\n",
    "   - For time-series data, use methods like linear or polynomial interpolation to estimate missing values based on surrounding data.\n",
    "\n",
    "5. **Using Algorithms That Handle Missing Data**:\n",
    "   - Some models, like decision trees, can handle missing data internally and make decisions without the need for imputation.\n",
    "\n",
    "Choosing the right method depends on the amount and nature of missing data, as well as the specific context of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c33c94-93fb-4c2a-9be0-b3476b493d4c",
   "metadata": {},
   "source": [
    "\n",
    "1. Missing Completely at random \n",
    "2. Missing At Random\n",
    "3. Missing not at Random\n",
    "   \n",
    "### We can handle missing data using Imputation.\n",
    "1. Numerical Data :- we will use mean & median Imputation\n",
    "2. Categorical Data :- we will use mode imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f202a5-2ec9-4430-bf93-56fb31a8d32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "664bcbb5-35dc-411d-b1e3-0123738fb459",
   "metadata": {},
   "source": [
    "## Q27. Explain the implications of ignoring missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce6b26-24d1-40ae-abdf-2f59cbb62eab",
   "metadata": {},
   "source": [
    "Implication of Missing Data:==>\n",
    "    \n",
    "1. sampling Bias\n",
    "    \n",
    "2. Missing data implicate on model performance\n",
    "    \n",
    "3. not getting generalize model or distorted patterns\n",
    "    \n",
    "4. problematic to find value able insight from data.\n",
    "\n",
    "5. Inefficient model\n",
    "\n",
    "6. Reduced model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39268a0d-8263-4c63-b8ee-0ad54ffa02e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fccf2d1-bf5a-40da-be19-94fa79965508",
   "metadata": {},
   "source": [
    "## Q28 . Discuss the pros and cons of imputation methods ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373dc8a-3fc8-47f4-bd34-6645adb7d813",
   "metadata": {},
   "source": [
    "There is 2 types of features:\n",
    "1. Numerical feature :if outlier treatment is done we can use mean imputation\n",
    "if outlier treatment has not done we can use median imputation.\n",
    "        \n",
    "2. categorical feature :\n",
    "here we can use mode imputation.\n",
    "    \n",
    "#### Pros and cons of imputation methods:\n",
    "\n",
    "1. **Mean/Median Imputation**:\n",
    "   - **Pros**: Simple, fast, works for small missing data.\n",
    "   - **Cons**: Distorts data distribution, reduces variability.\n",
    "\n",
    "2. **Mode Imputation**:\n",
    "   - **Pros**: Easy to implement for categorical data.\n",
    "   - **Cons**: Can bias results, reduces data variety.\n",
    "\n",
    "3. **KNN Imputation**:\n",
    "   - **Pros**: Considers relationships between features, accurate.\n",
    "   - **Cons**: Slow, computationally expensive, may not work well for sparse data.\n",
    "\n",
    "4. **Predictive Model Imputation**:\n",
    "   - **Pros**: Accurate, based on feature relationships.\n",
    "   - **Cons**: Complex, computationally heavy, may overfit.\n",
    "\n",
    "5. **Multiple Imputation**:\n",
    "   - **Pros**: Handles uncertainty, accurate.\n",
    "   - **Cons**: Complex, time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb9493-9c95-466e-b714-d7cb1cdffb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b899f8d8-ceb4-4b5c-8330-3d8314a90991",
   "metadata": {},
   "source": [
    "## Q29 How does missing data affect model Performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5b1c9-03e2-4566-a711-e12d58df3679",
   "metadata": {},
   "source": [
    "Due to missing data model will predict incorrect results.or sometime model will not understand data.\n",
    "\n",
    "Missing data can negatively impact model performance in several ways:\n",
    "\n",
    "1. **Bias**: If data is missing in a non-random way, it can introduce bias, making the model inaccurate.\n",
    "2. **Reduced Accuracy**: Missing values reduce the amount of data available for training, leading to less information for the model to learn from, which can lower its accuracy.\n",
    "3. **Distorted Relationships**: Missing data can hide important patterns and relationships, leading the model to make incorrect predictions.\n",
    "4. **Overfitting or Underfitting**: Incomplete data may cause models to overfit to the training data or underfit, resulting in poor generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d86dd-897d-4a75-b6d7-44daf1b340a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "432c43a3-df9c-4679-b266-f4e636b2f713",
   "metadata": {},
   "source": [
    "## Q30. Define imbalanced data in the context of machine learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b83f0-27dc-4197-a25c-89f7990915ca",
   "metadata": {},
   "source": [
    "Imbalanced data in machine learning refers to a situation where the classes in the dataset are not equally represented. \n",
    "When one class has higher percentage of data as compare to another class is called as imbalanced data\n",
    "    \n",
    "for example :\n",
    "        \n",
    "Majority class ==> 90%\n",
    "Minority class ==> 10%\n",
    "\n",
    "This can cause issues because the model may become biased towards predicting the majority class, ignoring the minority class. As a result, the model might perform poorly on the underrepresented class, leading to inaccurate predictions. Handling imbalanced data often requires techniques like resampling, weighting, or using specialized algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f122259-bb1e-4260-b649-1d5d749d5fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a952f71-5834-4efc-b852-9d8eef308de8",
   "metadata": {},
   "source": [
    "## Q31 Discuss the challenges posed by imbalanced data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6900c0-0d52-41ee-862f-5228c61ff0ad",
   "metadata": {},
   "source": [
    "Imbalanced data poses several challenges in machine learning:\n",
    "\n",
    "1. **Bias Toward Majority Class**: The model may favor the majority class, leading to poor performance on the minority class, which is often more important.\n",
    "2. **Poor Generalization**: The model may not learn the correct patterns for the minority class, causing inaccurate predictions.\n",
    "3. **Evaluation Issues**: Standard metrics like accuracy can be misleading, as the model might predict the majority class well while neglecting the minority class.\n",
    "4. **Difficulty in Learning**: The model may struggle to recognize the minority class due to fewer examples, affecting overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6aa4bb-1f58-4299-a559-b3144c4aa95c",
   "metadata": {},
   "source": [
    "Imbalance data can cause problem like biased model, inaccurate prediction. \n",
    "In case of classification, when one class is majority class & another class is Minority class so the classification result will be majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ebe4d-50b7-46a8-aa57-9285991da166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7ff8bb8-3e02-4e68-8fb2-9d7f3be8e97e",
   "metadata": {},
   "source": [
    "## Q32 What techniques can be used to address imbalanced data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db783c-5a1b-4d2a-86cf-9a953895e046",
   "metadata": {},
   "source": [
    "To address imbalanced data:\n",
    "\n",
    "1. **Resampling**: \n",
    "   - **Oversample** minority class or **undersample** majority class.\n",
    "   \n",
    "2. **Class Weights**: Assign higher weights to the minority class.\n",
    "\n",
    "3. **Synthetic Data**: Generate new data points for the minority class (e.g., **SMOTE(synthetic minority oversampling techniques)**).\n",
    "\n",
    "4. **Use Correct Metrics**: Evaluate with Precision, Recall, or F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee343a0-bf07-45c7-98f7-ae07b498c60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54114cf0-5659-4b77-bfd4-4b195e38c328",
   "metadata": {},
   "source": [
    "## Q33 Explain the process of up-sampling and down -sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fcc986-e1f8-4dff-bfd6-b536f954ce45",
   "metadata": {},
   "source": [
    "**Up-sampling** and **down-sampling** are techniques used to address imbalanced data:\n",
    "\n",
    "1. **Up-sampling**: Involves increasing the number of minority class samples by duplicating existing data or generating synthetic samples (e.g., **SMOTE**). This balances the dataset and helps the model learn from more minority class examples.\n",
    "- In up sampling repeat the data from majority class which is equivalent to minority class\n",
    "\n",
    "2. **Down-sampling**: Reduces the majority class by randomly removing some of its data points. This makes the dataset more balanced but may lose useful information from the majority class. \n",
    "- In down sampling use the data from majority class which equivalent to miniority class.\n",
    "\n",
    "Note : Both methods aim to improve model performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ea859-4492-415d-b737-55e3840680d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40d6bb9a-edf9-4918-9584-c78e28e7df1a",
   "metadata": {},
   "source": [
    "## Q34 When would you use up-sampling versus down-sampling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335178a-d807-4b6f-8570-6d9c59a9647c",
   "metadata": {},
   "source": [
    "If the dataset has an imbalanced distribution, **up-sampling** is often more effective as it increases the representation of the minority class \n",
    "without losing any information from the majority class. This is especially useful when the minority class is small but important, like detecting \n",
    "rare diseases. \n",
    "On the other hand, **down-sampling** reduces the majority class, which may lead to a loss of valuable data, especially if the majority class \n",
    "contains critical information. Down-sampling can be less effective in balancing class distribution when the majority class is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70880b6b-7dae-4ff5-8a62-f5c706928f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71b41462-6153-4cdc-8c29-b72c7c6cbc46",
   "metadata": {},
   "source": [
    "## Q35 What is SMOTE and How does it work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3693a1e-33ce-4288-9b94-4fbadf8c94a5",
   "metadata": {},
   "source": [
    "SMOTE (Synthetic Minority Over-sampling Technique) is a method used to address class imbalance in datasets. In SMOTE technique state that same nature of data will be part of same group. SMOTE is an oversampling technique that generates synthetic samples from the minority class. It obtains a synthetically class-balanced or nearly class-balanced training set, then trains the classifier.\n",
    "This helps balance the dataset and improve the performance of machine learning models by providing more varied examples of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70b373-d323-4707-84eb-0eb4aae4819e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "417927ed-37d1-4e45-9875-c0167220a22d",
   "metadata": {},
   "source": [
    "## Q36 Explain the role of SMOTE in handling imbalanced data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f340b68-cb14-400c-9b96-a2d0054deea5",
   "metadata": {},
   "source": [
    "SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesis new minority instances between existing minority instances.\n",
    "**How it Works**: SMOTE selects a data point from the minority class, finds its nearest neighbors, and creates new samples by blending features from the selected point and its neighbors.\n",
    "\n",
    "**Benefit**: This increases the minority class size, balancing the dataset and improving model performance, particularly for predicting the minority class accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932f63f-3247-49b6-b0ac-8af8f65253e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbb14fb2-aa4c-4b97-895c-fb3aed5e5780",
   "metadata": {},
   "source": [
    "## Q37. Discuss the advantages and limitations of SMOTE ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fada5-ed32-49f9-a8b3-74c1632a2433",
   "metadata": {},
   "source": [
    "A drawback of SMOTE is that it doesn't consider the majority class while creating synthetic samples. Additionally, it can create synthetic samples between samples that represent noise. As a result, the augmented dataset will have more noise than the original one, which can hurt performance.\n",
    "    \n",
    "While SMOTE is highly effective, it's not without its challenges and limitations: Data Quality: SMOTE assumes that the minority class instances are close in feature space. If the minority class is very sparse or if the data quality is poor, the synthetic samples created may not be representative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1899c7c-6cf4-4d5d-96a2-b2420e506334",
   "metadata": {},
   "source": [
    "## Advantages:\n",
    "1. **Improves Model Performance**: By balancing the dataset, SMOTE helps models better predict the minority class.\n",
    "2. **Synthetic Data**: Generates new, diverse examples, reducing overfitting compared to simple duplication of minority class samples.\n",
    "3. **Works for Various Algorithms**: Can be used with any machine learning model to handle class imbalance.\n",
    "\n",
    "## Limitations:\n",
    "1. **Overfitting Risk**: Generating too many synthetic samples might cause overfitting, especially if the original data is noisy.\n",
    "2. **Not Ideal for High-Dimensional Data**: In datasets with many features, SMOTE may create unrealistic or less useful synthetic examples.\n",
    "3. **Increased Computation**: Generates additional data, which can increase training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc25aca-5e25-4bc3-92a4-0d715ffc5929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "972d66c2-e2c4-4d70-a18d-d426e7f4c605",
   "metadata": {},
   "source": [
    "## Q38 Provide examples of scenarios where SMOTE is beneficial ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3629b-ec21-4d68-b8b3-2374b319250c",
   "metadata": {},
   "source": [
    "SMOTE creates new synthetic spam emails based on existing ones, balancing the dataset for better spam detection.\n",
    "1. **Medical Diagnosis**: SMOTE helps with rare diseases, generating more examples for better prediction of those cases.\n",
    "   \n",
    "2. **Fraud Detection**: In detecting fraud, fraudulent transactions are rare. SMOTE helps by balancing the dataset for better fraud detection.\n",
    "\n",
    "3. **Customer Churn Prediction**: For businesses with few customers leaving, SMOTE creates more churn examples, improving prediction accuracy.\n",
    "\n",
    "4. **Image Classification**: When rare objects are underrepresented in images, SMOTE generates synthetic examples, aiding in better recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1719c-b32f-4e8d-9cec-a39c770caf95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a4a0484-e01a-4322-86a8-550f2dd3281c",
   "metadata": {},
   "source": [
    "## Q39 Define data interpolation and its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ababa60-f16f-42e8-8b44-3d8cd6efaa3e",
   "metadata": {},
   "source": [
    "Interpolation is a process of determining the unknown values that lie in between the known data points. It is mostly used to predict the unknown values for any geographical related data points such as noise level, rainfall, elevation, and so on.\n",
    "**Purpose**: It helps create a complete dataset by filling in gaps, improving accuracy in analysis or predictions.\n",
    "\n",
    "**Use Cases**: Interpolation is commonly used in time-series data, sensor readings, and spatial data to make informed decisions or predictions, ensuring continuity and reducing errors caused by missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dafa7c-17e0-4d3a-b9a5-e1cc29452034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a2ee096-3437-4322-ba49-3ead91eac9d6",
   "metadata": {},
   "source": [
    "## Q40 What are the common methods of data interpolations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcaffd5-27d4-41a4-91b6-f26e0def61ef",
   "metadata": {},
   "source": [
    "There are three common method of data interpolation :\n",
    "1. **Linear interpolation** : Estimates missing values by connecting two known points with a straight line.\n",
    "\n",
    "2. **cubic interpolation** : Uses a cubic function to create a smoother curve between points, providing more accuracy than linear interpolation.\n",
    "        \n",
    "3. **Polynomial interpolation** : Fits a polynomial function to the data points, offering even more flexibility but can be prone to overfitting with complex data.\n",
    "\n",
    "4. **Nearest-Neighbor Interpolation** : Assigns missing values based on the closest known data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c6c5b-c28c-4ad1-aa16-9af8e9beae17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f68d7b96-102c-4b59-8d6a-892019fdc76a",
   "metadata": {},
   "source": [
    "## Q41 Discuss the implications of using data interpolation in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd868c-3adf-43af-95e9-027e23624e33",
   "metadata": {},
   "source": [
    "Interpolation refers to the process of estimating unknown values that fall between known data points. This can be useful in various scenarios, such as filling in missing values in a dataset or generating new data points to smooth out a curve.\n",
    "Overfitting can occur with complex methods like polynomial interpolation, where the model becomes too tailored to the data. It’s important to use interpolation carefully, as it assumes missing values follow existing trends, which might not always be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d68ed-4e73-4428-b505-a35b3516e65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06bd3cdb-eb79-4c39-b4eb-0e3f432623f5",
   "metadata": {},
   "source": [
    "## Q42. What are outliers in a dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b82c8-27a2-43ec-b1ba-04f55202075c",
   "metadata": {},
   "source": [
    "Outlier in dataset is most extreme values are present in dataset is called outliers. They are unusually high or low values that can distort analysis or model performance. Outliers may result from errors, rare events, or variability, and it's important to handle them carefully to avoid misleading results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6147c-d9f9-4bb2-86c6-bc4d9570bc10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b57366-6004-4fdc-89a5-11cb40af8ebb",
   "metadata": {},
   "source": [
    "## Q43 Explain the impact of outliers in machine learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1755cc-8f57-47d7-be96-54f2d3dbbc5b",
   "metadata": {},
   "source": [
    "Outlier impact in machine learning model :\n",
    "        \n",
    "    1.Model can be biased due to outliers\n",
    "    \n",
    "    2.model cant give accurate prediction.\n",
    "    \n",
    "    3.reliability\n",
    "\n",
    "    4.Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862f61c-34f4-4096-9b81-f50188ca73ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0d7299-c6f6-46e7-bb31-4a0101d7ffb1",
   "metadata": {},
   "source": [
    "## Q44 Discuss techniques for identifying outliers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4aa0f-cf6f-440c-b2a3-4cd21320e432",
   "metadata": {},
   "source": [
    "1. **Z-Score**: Measures how far a data point is from the mean in terms of standard deviations. A high z-score indicates an outlier. Provides summary statistics (mean, std, min, max) to identify values far from the central range, indicating potential outliers.\n",
    "\n",
    "2. **IQR (Interquartile Range)**: Identifies outliers by calculating the range between the 25th and 75th percentiles and flagging values outside this range.\n",
    "\n",
    "3. **Visualization**: Box plots or scatter plots visually highlight outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73811a1-8c9e-4e9e-8f98-0c1bc70fe064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82a6d677-9713-4c02-abe3-e466fc65ff9e",
   "metadata": {},
   "source": [
    "## Q45 How can outliers be handled in a dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d92be2-1086-4a82-ba69-78512d422d13",
   "metadata": {},
   "source": [
    "1. **Remove Outliers** : If we want to removed extreme values from data we need to use 5 point summary (min, Q1, median, Q3, max) and the IQR (Interquartile Range) for outlier treatment we use IQR range find outliers & removed it.\n",
    "\n",
    "2. **Impute Outliers** : if we don't want remove outlier from data we can impute it with mean and median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd0251-8cf9-483e-ab98-3fb117f2da75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e691b655-d1b2-4f8d-9461-6a7cf28ec591",
   "metadata": {},
   "source": [
    "## Q46 Compare and contrast Filter ,Wrapper, and Embedded methods for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865a5dd-6b45-46d7-b32e-5112bba2e53f",
   "metadata": {},
   "source": [
    "#### Filter methods\n",
    "Evaluate features independently of the model, based on their individual characteristics and statistical tests. They're fast and good for large datasets, but don't consider feature relationships. Examples include variance thresholds and information gain.\n",
    "\n",
    "#### Wrapper methods\n",
    "Test different combinations of features to see which performs best for a specific model. They can offer better model performance, but are computationally intensive and time-consuming. Examples include sequential feature selection.\n",
    "\n",
    "#### Embedded methods\n",
    "Select features while training the model itself. They balance efficiency and model-specific learning, and are less prone to overfitting. Examples include LASSO and Ridge.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb3e92-bc57-47ec-bf41-4cd3bb631333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d39eed62-4095-465d-9f85-3a38ff603759",
   "metadata": {},
   "source": [
    "## Q47 Provide examples of algorithms associated with each method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796c150-f577-47f1-9eac-249a8608b47d",
   "metadata": {},
   "source": [
    "1. **Filter Methods**: Examples include **Chi-Square**, **ANOVA**, and **Correlation-based** methods.\n",
    "\n",
    "2. **Wrapper Methods**: Examples include **Recursive Feature Elimination (RFE)** and **Forward Selection**.\n",
    "\n",
    "3. **Embedded Methods**: Examples include **Lasso Regression** (L1 regularization), **Ridge Regression** (L2 regularization), and **Elastic Net Regression** (L1 + L2 regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ebd9f-ccfd-4adb-b752-cba1d90eda99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8ff00fd-84da-4293-a4a4-a649ed99cc2a",
   "metadata": {},
   "source": [
    "#### Q48 Discuss the advantages and disadvantages of each feature selection method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f01140-3c71-420c-a046-481e487d75e8",
   "metadata": {},
   "source": [
    "#### Filter Method :\n",
    "        \n",
    "- Advantage :\n",
    "\n",
    "    1.computationaly efficient,fast processing ,less prone to overfitting.\n",
    "    \n",
    "- Disadvantage:\n",
    "    \n",
    "    1.less precision could fail to find the feature.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c51d9d-b7f6-45bb-9d27-e6d4e8467c8b",
   "metadata": {},
   "source": [
    "#### Wrapper method :\n",
    "\n",
    "- Advantages :\n",
    "\n",
    "    1.high precision\n",
    "\n",
    "- Disadvantage :\n",
    "\n",
    "    1.computationally expensive, less precision,tend to over fitting\n",
    "\n",
    "\n",
    "#### Embedded method\n",
    "\n",
    "- Advantage :\n",
    "\n",
    "    1.Low extra cost \n",
    "\n",
    "- Disadvantage\n",
    "\n",
    "    1.Learning dependant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62314a-b980-404b-ba5b-bea3a8651c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3446a419-b456-4724-8cd2-00f15054e6e6",
   "metadata": {},
   "source": [
    "## Q49 Explain the concept of feature scaling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1c9c1-118c-45ba-9084-f6a37c3b9da8",
   "metadata": {},
   "source": [
    "Feature scaling state that the feature will bring on same scale for modelling purpose. Feature scaling is the process of normalizing or standardizing features in a dataset to ensure they have a similar range or distribution. This helps machine learning models perform better by preventing features with larger values from dominating the learning process. Common methods include Min-Max Scaling and Standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4c8f9-6b6e-4cce-a8a5-0cd6b288bae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d37a0c92-048a-4e5a-b1cb-31dfca2e0fdf",
   "metadata": {},
   "source": [
    "## Q50 Describe the process of standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3d060-47aa-4a91-9e50-eecd1b00ec60",
   "metadata": {},
   "source": [
    "Standardization is a statistical technique used in data preprocessing to make different variables more comparable. It's like translating all these different data “languages” into one universal dialect`\n",
    "Standardization is a data preprocessing technique that transforms features to have a mean of 0 and a standard deviation of 1. It is done by subtracting the mean and dividing by the standard deviation, making features more comparable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de56efb-b1ae-4f4a-92da-2ac7c39b84cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f3289d9-6a4d-4564-b6a0-f8625b34f4d8",
   "metadata": {},
   "source": [
    "## Q51 How does mean normalization differ from standardization ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081bccc0-082a-48e9-93aa-f37b6d4e0077",
   "metadata": {},
   "source": [
    "Mean normalization and standardization are both scaling techniques, but they differ in how they adjust data:\n",
    "\n",
    "#### Mean Normalization :\n",
    "        \n",
    "- Scales data by subtracting the mean and dividing by the range (max - min), making the data fall between -1 and 1. \n",
    "- In normalization minimum & maximum value are use for scaling.\n",
    "        \n",
    "#### standardization :\n",
    "        \n",
    "- Scales data by subtracting the mean and dividing by the standard deviation, resulting in a mean of 0 and a standard deviation of 1.\n",
    "- In standardization mean & standard deviation is used for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf26d84-e471-49c9-9a66-a138a13e66f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0473b558-85f7-4cd2-8af7-2762bce1a5ff",
   "metadata": {},
   "source": [
    "## Q52 Discuss the advantages and disadvantages of Min-Max scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092c0ff-7a8a-4cd2-9c98-b2ec47a04852",
   "metadata": {},
   "source": [
    "#### Advantage :   \n",
    "    1.Simple to understand and implement.\n",
    "    2.Effective for a wide range of data.\n",
    "    \n",
    "#### Disadvantage :   \n",
    "    1.Sensitive to outlier\n",
    "    2.can lead to information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8fa27-c2a1-4672-86c6-0148b2bac63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a0ce87-b85e-48fd-875d-c00dea7c0de6",
   "metadata": {},
   "source": [
    "## Q53 What is the Purpose of unit vector scaling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342785f-b322-40ab-bde0-f172ea6caeb0",
   "metadata": {},
   "source": [
    "Unit vector scaling is a technique that can be used to normalize the range of independent variables or features of data. It's also known as feature scaling and is often performed during data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3fb2a-c491-40d0-8cd0-2d185010fbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "026b4ee7-0ca0-431a-ad5a-40a067080f02",
   "metadata": {},
   "source": [
    "## Q54 Define Principle Component Analysis (PCA)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671034e-0d0f-457f-876a-c881b3310d83",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a dimensionality reduction and machine learning method used to simplify a large data set into a smaller set while still maintaining significant patterns and trends. It transforms the original data into new variables, called principal components, that capture the most significant patterns or variations, making the data easier to analyze and visualize.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7dfba7-470c-470f-bfd8-602f32ee190e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75bc7d79-208f-4bc7-a164-dd33fdfc6069",
   "metadata": {},
   "source": [
    "## Q55 Explain the steps involved in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1c386-953d-47e9-9110-66095dc2c74d",
   "metadata": {},
   "source": [
    "1. **Standardize the Data**: Scale features to have zero mean and unit variance.\n",
    "2. **Compute Covariance Matrix**: Identify relationships between features.\n",
    "3. **Find Eigenvectors and Eigenvalues**: Determine the principal components.\n",
    "4. **Select Principal Components**: Choose components with the highest variance.\n",
    "5. **Transform the Data**: Project the data onto the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00840643-09a8-4d49-98f5-bd2f0f8cf693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d563237-109a-4b78-a9df-daa9667c94bd",
   "metadata": {},
   "source": [
    "## Q56 Discuss the significance of eigenvalues and eigenvectors in PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73b668-1de1-48de-80ae-e69c08145dac",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are crucial in PCA. Eigenvalues indicate the amount of variance captured by each principal component, while eigenvectors represent the direction \n",
    "of these components. The higher the eigenvalue, the more significant the corresponding eigenvector. These components help reduce data dimensionality while preserving \n",
    "important patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb10332-f192-4caf-ba64-eb91b6249f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93e4fad1-d570-4496-aa1a-532640aa05b3",
   "metadata": {},
   "source": [
    "## Q57 How does PCA helps in dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2ad30-b6fc-4746-9e05-5673c0aae199",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a linear dimensionality reduction technique that reduces the number of dimensions in a data set while retaining most of its information. It does this by transforming the original variables into a smaller set of new, uncorrelated variables called principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25868fe3-e720-4d6b-a101-4901b344c560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b45fc145-1042-430e-be37-56530518ebe6",
   "metadata": {},
   "source": [
    "## Q58 Define data encoding and its importance in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478aec83-07d4-4cb4-837d-43b1d5557320",
   "metadata": {},
   "source": [
    "Data encoding is the process of converting data, usually categorical or text data, into a numerical format that machines can understand and process. It's a crucial step in preparing data for machine learning algorithms, as these algorithms primarily work with numerical data.\n",
    "It is important because most algorithms require numerical input, and encoding ensures the data can be efficiently processed, improving model accuracy and performance while maintaining the relationship between categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb3c3e-5512-413d-b43b-7c7e601850de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9681ff8b-0cea-40ac-a527-baa68fca7052",
   "metadata": {},
   "source": [
    "## Q59 Explain Nominal Encoding and provide an example ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4858048-4552-41aa-aadf-a6c061fc638d",
   "metadata": {},
   "source": [
    "In nominal encoding convert categorical data into numerical without implying any order or ranking. This is used for features where categories have no inherent relationship.\n",
    "    \n",
    "Example : Gender (Male, Female) or Marital Status (Single, Married) can be encoded as 0 and 1, respectively, without implying any ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd23bd0-a283-4264-be36-761c7bde951e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0bb0dc0-bc66-4e21-8b1c-1d3def1729b8",
   "metadata": {},
   "source": [
    "## Q60 Discuss the process of One Hot Encoding ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70853631-0306-4ed7-8b6f-bcef9ac4d5a0",
   "metadata": {},
   "source": [
    "- In onehot Encoding Categorical data convert into numerical data. \n",
    "- There is no order in data.\n",
    "    \n",
    "Example : Single, Married, In relationship\n",
    "    \n",
    "    \n",
    "| Single | Married | In Relationship |\n",
    "|--------|---------|-----------------|\n",
    "|   1    |    0    |        0        |\n",
    "|   0    |    1    |        0        |\n",
    "|   0    |    0    |        1        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a2eb8-0b47-433e-a1c0-d55760b67dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "867beeeb-50b6-4c07-bb67-748d3800120b",
   "metadata": {},
   "source": [
    "## Q61 How do you handle multiple categories in One Hot Encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ebe77-2592-4991-8b52-285b43949079",
   "metadata": {},
   "source": [
    "One-hot encoding is a technique that converts categorical variables into numerical variables by creating a new binary variable for each possible value of the categorical variable. When dealing with multiple categories, you can:\n",
    "Limit encoding to frequent labels\n",
    "Encode only the 10 most frequent labels, and group all other labels under a new category.\n",
    "Drop a dummy variable\n",
    "Since dummy variables contain redundant information, you can drop one of the newly created columns to avoid the dummy variable trap.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adbfe1-e8e7-4c6f-83c3-63a5d138743a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dc8430d-d88a-4463-8346-17d507c9b2aa",
   "metadata": {},
   "source": [
    "## Q62 Explain Mean Encoding and its advantages ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0c820-8b0f-46a8-9192-db26d522e27c",
   "metadata": {},
   "source": [
    "Target encoding, also known as mean encoding, involves replacing each category with the mean (or some other statistic) of the target variable for that category. Here's how target encoding works: Calculate the mean of the target variable for each category. Replace the category with its corresponding mean value.\n",
    "### Advantages:\n",
    "\n",
    "1. Captures more information than one-hot encoding.\n",
    "2. Reduces dimensionality compared to other encoding methods.\n",
    "3. Can improve model performance by incorporating target relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c54cb5-2376-48f7-85da-d1dfa62ec7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ade4c65-3cf0-47d8-93cb-1044689620ee",
   "metadata": {},
   "source": [
    "## Q63 Provide examples of Ordinal Encoding and Label Encoding ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60e41c-60bb-4408-9a14-774055c5d4eb",
   "metadata": {},
   "source": [
    "**Ordinal Encoding**: Used for features with a natural order. For example, rating levels like \"Low\", \"Medium\", and \"High\" can be encoded as 0, 1, and 2, respectively.\n",
    "\n",
    "**Label Encoding**: Converts categories into numerical labels without implying any order. For example, \"Red\", \"Green\", and \"Blue\" can be encoded as 0, 1, and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ecd614-0f63-46e6-a7df-5103f9e041c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "383bc594-f93b-45f2-a126-1cfd29eece14",
   "metadata": {},
   "source": [
    "## Q64 What is Target Guided Ordinal Encoding and how is it used ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fc67a-8c96-4839-ac1e-e66fbc1c73db",
   "metadata": {},
   "source": [
    "Target-guided ordinal encoding is a technique used to encode categorical variables for machine learning models. This encoding technique is particularly useful when the target variable is ordinal, meaning that it has a natural order, such as low, medium, and high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8008f-bc4a-4f60-ab0c-7553f59c7a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df30eafc-f080-4423-b988-32f4c5331d0c",
   "metadata": {},
   "source": [
    "## Q65 Define covariance and its significance in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44979629-012e-427e-8bf2-49e6a20548be",
   "metadata": {},
   "source": [
    "**Covariance** is a statistical measure that indicates the relationship between two random variables, showing how they vary together. A positive covariance suggests that the variables tend to increase or decrease in tandem, while a negative covariance indicates an inverse relationship. However, covariance does not indicate the strength of the relationship or the degree of dependency between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ad613-6371-4d27-a3fa-3fa46ac745cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf0c755-0c0d-45d0-b9d6-6a4c85fd1648",
   "metadata": {},
   "source": [
    "## Q66 Explain the process of correlation check ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a27c78-cf09-4b9d-a432-aaf354d4f634",
   "metadata": {},
   "source": [
    "**Correlation check** is used to assess the strength and direction of a linear relationship between two random variables. It can be positive, negative, or zero, indicating how the variables move together. In Python, correlation can be computed using the corr() method or visualized through a heatmap to analyze the linear relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d29fe-d5b1-44d0-91f3-c4441714876d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e85f8bb-3c10-457c-be42-a0fab3bbd785",
   "metadata": {},
   "source": [
    "## Q67 What is the Pearson Correlation Coefficient ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc06cc6-79ef-4a3c-851a-fd8ab8dd9662",
   "metadata": {},
   "source": [
    "The Pearson correlation coefficient (r) is the most common way of measuring a linear correlation. It ranges from –1 and +1 that measures the strength and direction of the relationship between two variables.\n",
    "where +1 indicates a perfect positive correlation, -1 represents a perfect negative correlation, and 0 suggests no linear relationship. It is widely used in statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8afd9-0040-4bdc-a291-4bf86ef9a850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112d5699-41c3-4370-80f9-9c321dfa7fc0",
   "metadata": {},
   "source": [
    "## Q68 How does Spearmans Rank Correlation differ from Pearson's Correlation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f7156-221f-4c19-9594-17a68532375c",
   "metadata": {},
   "source": [
    "- Spearman's rank correlation:\n",
    "\n",
    "A nonparametric measure that assesses how well a monotonic function describes the relationship between two variables. It's often used for ordinal data, non-normally distributed continuous data, or data with outliers. Spearman's correlation orders values from highest to lowest, without considering the distances between them. A perfect Spearman correlation of +1 or −1 occurs when each variable is a perfect monotone function of the other.\n",
    "It is used when the data is not linearly related or is ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3793164-77c1-48a6-bd29-ef8f6c9f5afa",
   "metadata": {},
   "source": [
    "- Pearson's correlation\n",
    "\n",
    "\n",
    "Assesses linear relationships between quantitative variables that follow a normal distribution. It's typically used for jointly normally distributed data. Pearson's correlation compares the mean value of the product of the standard scores of matched pairs of observations. Positive values indicate a positive correlation, negative values indicate a negative correlation, and zero values indicate no correlation. \n",
    "Pearson requires data to be continuous and normally distributed, while Spearman is more flexible and can handle non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3309d1-aa00-43ed-96b8-badb195b326d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d225ad59-0b46-4ad1-aca4-0ab281021a5f",
   "metadata": {},
   "source": [
    "## Q69 Discuss the importance of Variance Inflation Factor (VIF) in feature selection ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe4746-52e4-4a26-ba9b-97ed243c1d54",
   "metadata": {},
   "source": [
    "The variance inflation factor (VIF) is a statistical tool that measures how much the variance of an estimated regression coefficient is increased due to collinearity. It's a useful tool for feature selection in machine learning analysis because it can help identify and eliminate variables that cause multicollinearity. Multicollinearity can lead to unstable parameter estimation, weak predictive ability, and less dependable statistical conclusions. Removing multicollinearity can improve the accuracy and sensitivity of classification models, and the stability and generalization performance of extreme learning machines (ELM) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195f1f8-a7a0-4468-b98b-e19ff9a092c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ddd29b4-c7de-446a-a35d-b70e6fa519b7",
   "metadata": {},
   "source": [
    "## Q70 Define feature selection and its purpose ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363f146-3435-4cc3-8402-1b6e4095229a",
   "metadata": {},
   "source": [
    " The goal of feature selection is to find the best set of features from the available data that models the given problem to yield a machine learning model with good performance and robustness.\n",
    "The goal is to improve model performance, reduce overfitting, and enhance interpretability by eliminating irrelevant or redundant features, leading to a more efficient and robust machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e470a-ce23-4813-8763-ce6dfdad3c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b16ed7eb-39ed-4305-82e9-e2dbd9031b4c",
   "metadata": {},
   "source": [
    "## Q71 Explain the process of Recursive Feature Elimination ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de762a-3122-4d04-8858-6d349baf34e0",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (RFE) is a feature selection method to identify a dataset's key features. The process involves developing a model with the remaining features after repeatedly removing the least significant parts until the desired number of features is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a33a56-a4eb-4513-8ef2-87c5c1f8df85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcd4852b-dc32-47bd-b0c8-fa3c8fb941fc",
   "metadata": {},
   "source": [
    "## Q72 How does Backward Elimination work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2456e-0a7c-404e-9606-3762c379dfb4",
   "metadata": {},
   "source": [
    "Backward elimination is a more systematic approach that starts with a complete set of features and removes features one by one until the model (e.g., p-values in regression) performance reaches a peak. This method is more computationally efficient but may not find the optimal set of features either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ae4b3-c094-493f-8a5b-d8f6f45d887b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f46b3eb-1de5-44b2-97ee-25f9c1a0b02d",
   "metadata": {},
   "source": [
    "## Q73 Discuss the advantages and limitations of Forward Elimination ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029df8d8-8635-44e6-a713-fe521cc16404",
   "metadata": {},
   "source": [
    "- Advantages :\n",
    "\n",
    "1. Simple and easy to understand.\n",
    "2. Builds the model progressively, adding the most significant features first.\n",
    "3. Can help improve model performance by selecting only relevant features.\n",
    "    \n",
    "- Limitation:\n",
    "\n",
    "1. Computationally expensive with many features.\n",
    "2. May overlook feature interactions and higher-order relationships.\n",
    "3. Prone to overfitting if not carefully validated, as it may select features that only fit the training data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8872de-b9a1-42d4-82a1-d61eefe48646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88686a3-25a6-45f5-a3db-cc336927c24f",
   "metadata": {},
   "source": [
    "## Q74 What is feature engineering and why is it important ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e61d1-4d91-4968-bc54-e09acac44d81",
   "metadata": {},
   "source": [
    "Feature engineering is a crucial step in machine learning that involves selecting and transforming raw data into features that can better represent an underlying problem for a predictive model. The goal is to improve model accuracy by providing more relevant and meaningful information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b28e1-5872-4c95-9278-728a6091ead4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7501634-8905-4961-9eff-813c69541979",
   "metadata": {},
   "source": [
    "## Q75 Discuss the steps involved in feature engineering ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac0f9b-2787-455f-83e8-ddeb58a5165c",
   "metadata": {},
   "source": [
    "Step in feature engineering\n",
    "    \n",
    "    1. Handle unique value\n",
    "    2. Handle Missing (NaN) values\n",
    "    3. Handle outliers\n",
    "    4. Encoding Categorical Variables\n",
    "    5. Feature Aggregation\n",
    "    6. Feature selection\n",
    "    7. Exploratory Data Analysis (EDA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cce3c8-a626-4ef2-a7e8-466a0118e1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "927193b2-f0e4-45c9-be0f-c90d05e304a8",
   "metadata": {},
   "source": [
    "## Q76 Provide example of feature engineering techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a3b48-476d-48af-9b41-00520daf6bd1",
   "metadata": {},
   "source": [
    "### Feature Engineering Techniques:\n",
    "\n",
    "1. **Feature Extraction**: Creating new features by extracting useful information from existing data.  \n",
    "   - **Example**: From a \"Date\" column, you might extract features like the day of the week, month, or year.\n",
    "\n",
    "2. **Imputation**: Filling missing values in a dataset with suitable replacements (mean, median, mode, or predicted values).  \n",
    "   - **Example**: If a \"Price\" column has missing values, you might replace them with the median price.\n",
    "\n",
    "3. **Scaling**: Standardizing or normalizing features to ensure all values are within a similar range, improving model performance.  \n",
    "   - **Example**: Applying Min-Max scaling to rescale \"Age\" to a [0, 1] range.\n",
    "\n",
    "4. **Handling Missing Values**: Identifying and dealing with missing data, typically through imputation or removal.  \n",
    "   - **Example**: Filling missing data in the \"Age\" column with the mean or using models to predict missing values.\n",
    "\n",
    "5. **Handling Outliers**: Identifying and managing extreme values that may distort model training.  \n",
    "   - **Example**: Capping extreme values in the \"Income\" column to a reasonable threshold.\n",
    "\n",
    "6. **One-Hot Encoding**: Converting categorical variables into binary columns.  \n",
    "   - **Example**: For a \"Color\" feature with values (\"Red\", \"Blue\", \"Green\"), create three binary columns: `Color_Red`, `Color_Blue`, `Color_Green`.\n",
    "\n",
    "7. **Log Transformation**: Applying a logarithmic transformation to skewed data to normalize the distribution.  \n",
    "   - **Example**: Applying a log transformation to the \"Price\" feature to reduce skew and make it more normally distributed.\n",
    "\n",
    "### Adjustments:\n",
    "- **Imputation** and **Handling Missing Values** are similar, but \"Imputation\" specifically refers to the process of filling missing values, whereas \"Handling Missing Values\" includes both imputation and removal.\n",
    "- **Feature extraction** is more about creating new features than just transforming or scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a117989-500a-44cf-b490-cb542aab379a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bf3089f-4049-460a-ab23-4362f1d87c10",
   "metadata": {},
   "source": [
    "## Q77 How does feature selection differ from feature engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac85f73-92b6-4eee-a281-dd76eaf4b5e0",
   "metadata": {},
   "source": [
    "**Feature engineering** and **feature selection** are both crucial in machine learning, but they serve different purposes:\n",
    "\n",
    "- **Feature Engineering** involves creating new features or transforming existing ones to better represent the underlying patterns in the data, improving model accuracy.\n",
    "- **Feature Selection** focuses on identifying and retaining the most relevant features, removing redundant or irrelevant ones to reduce model complexity and overfitting.\n",
    "\n",
    "While their objectives differ, both techniques are often used together to optimize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff567bfe-4b59-43c9-b5ed-409e8186f566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e327e468-a1c2-4978-a53a-c2f931f82388",
   "metadata": {},
   "source": [
    "## Q78 Explain the importance of feature selection in machine learning pipelines ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67045723-6527-42ff-a311-44de76ce2e22",
   "metadata": {},
   "source": [
    "In the machine learning process, feature selection is used to make the process more accurate. It also increases the prediction power of the algorithms by selecting the most critical variables and eliminating the redundant and irrelevant ones. This is why feature selection is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6b0fc-9f94-4b83-9b0a-ace8dbe79655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6b38367-4f1b-4ac4-a26c-af55c02ddcdc",
   "metadata": {},
   "source": [
    "## Q79 Discuss the impact of feature selection on model performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23968743-2e59-4614-8656-75d492fa5d1a",
   "metadata": {},
   "source": [
    "In the machine learning process, feature selection is used to make the process more accurate. It also increases the prediction power of the algorithms by selecting the most critical variables and eliminating the redundant and irrelevant ones. This is why feature selection is important.\n",
    "This helps reduce overfitting, increase generalization, and improve the model's accuracy. By reducing the number of features, it also decreases computational complexity and speeds up training, leading to more efficient models with better prediction power and reduced risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02769a2d-6cd1-4bf6-9ce9-589ba73f707b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb458e7a-f6fc-461b-8b38-96107cd3c39d",
   "metadata": {},
   "source": [
    "## Q80 How do you determine which features to include in a machine-learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d742b08-54e5-400b-9efb-8f57fcc0a312",
   "metadata": {},
   "source": [
    "**Determining which features to include in a machine learning model**:\n",
    "\n",
    "There are several methods for selecting the best features, including:\n",
    "\n",
    "1. **Variance Inflation Factor (VIF)**: Measures multicollinearity by assessing how much the variance of a feature is inflated due to its correlation with other features. Features with high VIF values are considered redundant and may be removed.\n",
    "\n",
    "2. **Recursive Feature Elimination (RFE)**: A method that recursively removes the least important features based on model performance, identifying the most relevant features.\n",
    "\n",
    "Other methods, such as **correlation analysis**, **feature importance from models like decision trees**, or **L1 regularization (Lasso)**, can also be used to select relevant features.\n",
    "\n",
    "Using a combination of these techniques helps ensure that the features included in the model are the most informative and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb6c7f-1310-4010-a360-b6eed88866d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e3360-03ba-4ea2-93cd-08ad00c74497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
